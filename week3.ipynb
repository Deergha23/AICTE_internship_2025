{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0j5fFZh5uSNnVxKzgG2Os",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deergha23/AICTE_internship_2025/blob/main/week3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UnkWR7vx5PEt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import gradio as gr\n",
        "from flask import Flask, request, jsonify\n",
        "import werkzeug.utils\n",
        "import zipfile\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def prepare_dataset():\n",
        "    \"\"\"Load dataset from Google Drive and create train/val splits\"\"\"\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Path to your dataset zip file in Google Drive\n",
        "    zip_file_path = '/content/drive/My Drive/downloads/archive.zip'\n",
        "    extraction_path = '/content/dataset/'\n",
        "\n",
        "    # Extract the dataset\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extraction_path)\n",
        "\n",
        "    # Verify extraction\n",
        "    extracted_files = os.listdir(extraction_path)\n",
        "    print(\"Extracted files/folders:\", extracted_files)\n",
        "\n",
        "    # Check if dataset is already split\n",
        "    if 'train' in extracted_files and 'val' in extracted_files:\n",
        "        train_data_path = os.path.join(extraction_path, 'train')\n",
        "        val_data_path = os.path.join(extraction_path, 'val')\n",
        "        return train_data_path, val_data_path\n",
        "\n",
        "    # If no train/val split, create it automatically\n",
        "    print(\"No train/validation split found - creating one automatically...\")\n",
        "    all_data_path = os.path.join(extraction_path, 'TrashType_Image_Dataset')  # Adjusted to point to the correct folder\n",
        "    class_folders = [f for f in os.listdir(all_data_path) if os.path.isdir(os.path.join(all_data_path, f))]\n",
        "\n",
        "    # Create train and val directories\n",
        "    train_path = os.path.join(extraction_path, 'train')\n",
        "    val_path = os.path.join(extraction_path, 'val')\n",
        "    os.makedirs(train_path, exist_ok=True)\n",
        "    os.makedirs(val_path, exist_ok=True)\n",
        "\n",
        "    # Split each class into train/val (80/20)\n",
        "    for class_folder in class_folders:\n",
        "        # Create class directories in train and val\n",
        "        os.makedirs(os.path.join(train_path, class_folder), exist_ok=True)\n",
        "        os.makedirs(os.path.join(val_path, class_folder), exist_ok=True)\n",
        "\n",
        "        # Get all files in class folder\n",
        "        class_files = os.listdir(os.path.join(all_data_path, class_folder))\n",
        "        train_files, val_files = train_test_split(class_files, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Copy files to train and val directories\n",
        "        for file in train_files:\n",
        "            src = os.path.join(all_data_path, class_folder, file)\n",
        "            dst = os.path.join(train_path, class_folder, file)\n",
        "            shutil.copy(src, dst)\n",
        "\n",
        "        for file in val_files:\n",
        "            src = os.path.join(all_data_path, class_folder, file)\n",
        "            dst = os.path.join(val_path, class_folder, file)\n",
        "            shutil.copy(src, dst)\n",
        "\n",
        "    print(f\"Created train/validation split in {train_path} and {val_path}\")\n",
        "    return train_path, val_path\n"
      ],
      "metadata": {
        "id": "nMYoiO-k7n-q"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path, val_path = prepare_dataset()\n",
        "print(f\"Training data: {train_path}\")\n",
        "print(f\"Validation data: {val_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eECXtufc9PrD",
        "outputId": "b4949305-1a24-46e2-8f0c-a3b2d71f7140"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Extracted files/folders: ['train', 'TrashType_Image_Dataset', 'val']\n",
            "Training data: /content/dataset/train\n",
            "Validation data: /content/dataset/val\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create Flask app\n",
        "app = Flask(__name__)\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 15\n",
        "LEARNING_RATE = 0.0001\n",
        "\n",
        "# ====================== MODEL TRAINING & EVALUATION ======================\n",
        "def build_and_train_model(train_data_path, val_data_path):\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_data_path,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_generator = val_datagen.flow_from_directory(\n",
        "        val_data_path,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    class_names = list(train_generator.class_indices.keys())\n",
        "\n",
        "    # Model architecture\n",
        "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    base_model.trainable = False\n",
        "\n",
        "    inputs = layers.Input(shape=(224, 224, 3))\n",
        "    x = base_model(inputs)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(512, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    outputs = layers.Dense(len(class_names), activation='softmax')(x)\n",
        "    model = models.Model(inputs, outputs)\n",
        "\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Training callbacks\n",
        "    callbacks_list = [\n",
        "        callbacks.ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_accuracy'),\n",
        "        callbacks.EarlyStopping(monitor='val_accuracy', patience=10)\n",
        "    ]\n",
        "\n",
        "    # Training\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=train_generator.samples // 32,\n",
        "        validation_data=val_generator,\n",
        "        validation_steps=val_generator.samples // 32,\n",
        "        epochs=15,\n",
        "        callbacks=callbacks_list\n",
        "    )\n",
        "\n",
        "    # Evaluation on training and validation sets\n",
        "    train_loss, train_acc = model.evaluate(train_generator)\n",
        "    val_loss, val_acc = model.evaluate(val_generator)\n",
        "\n",
        "    print(\"\\nModel Performance:\")\n",
        "    print(f\"Training Accuracy: {train_acc*100:.2f}%\")\n",
        "    print(f\"Training Loss: {train_loss:.4f}\")\n",
        "    print(f\"Validation Accuracy: {val_acc*100:.2f}%\")\n",
        "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "    model.save('garbage_classifier_final.keras')\n",
        "    with open('class_names.txt', 'w') as f:\n",
        "        f.write('\\n'.join(class_names))\n",
        "\n",
        "    return model, class_names, history\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QdoVGQK49XFG"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Load and train model\n",
        "    train_path, val_path = prepare_dataset()\n",
        "    model, class_names, history = build_and_train_model(train_path, val_path)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lG9P42y-l-6",
        "outputId": "040d88a2-0379-47d1-82ae-a595ca834126"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Extracted files/folders: ['train', 'TrashType_Image_Dataset', 'val']\n",
            "Found 2019 images belonging to 7 classes.\n",
            "Found 508 images belonging to 7 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 2s/step - accuracy: 0.4806 - loss: 1.5429 - val_accuracy: 0.8000 - val_loss: 0.5405\n",
            "Epoch 2/15\n",
            "\u001b[1m 1/63\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 1s/step - accuracy: 0.6250 - loss: 0.9171"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 646ms/step - accuracy: 0.6250 - loss: 0.9171 - val_accuracy: 0.8042 - val_loss: 0.5371\n",
            "Epoch 3/15\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 2s/step - accuracy: 0.7007 - loss: 0.7941 - val_accuracy: 0.8146 - val_loss: 0.5019\n",
            "Epoch 4/15\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 306ms/step - accuracy: 0.7188 - loss: 0.7138 - val_accuracy: 0.8146 - val_loss: 0.5074\n",
            "Epoch 5/15\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 2s/step - accuracy: 0.7013 - loss: 0.7856 - val_accuracy: 0.8354 - val_loss: 0.4590\n",
            "Epoch 6/15\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 636ms/step - accuracy: 0.7812 - loss: 0.5720 - val_accuracy: 0.8354 - val_loss: 0.4607\n",
            "Epoch 7/15\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 2s/step - accuracy: 0.7543 - loss: 0.7003 - val_accuracy: 0.8438 - val_loss: 0.4552\n",
            "Epoch 8/15\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 612ms/step - accuracy: 0.7188 - loss: 0.6368 - val_accuracy: 0.8625 - val_loss: 0.4557\n",
            "Epoch 9/15\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - accuracy: 0.7464 - loss: 0.6557 - val_accuracy: 0.8667 - val_loss: 0.4082\n",
            "Epoch 10/15\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 319ms/step - accuracy: 0.6875 - loss: 0.6976 - val_accuracy: 0.8729 - val_loss: 0.4106\n",
            "Epoch 11/15\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - accuracy: 0.7762 - loss: 0.6267 - val_accuracy: 0.8646 - val_loss: 0.4196\n",
            "Epoch 12/15\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 643ms/step - accuracy: 0.7500 - loss: 0.5167 - val_accuracy: 0.8667 - val_loss: 0.4064\n",
            "Epoch 13/15\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - accuracy: 0.7732 - loss: 0.5922 - val_accuracy: 0.8500 - val_loss: 0.4266\n",
            "Epoch 14/15\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 310ms/step - accuracy: 0.5938 - loss: 0.7775 - val_accuracy: 0.8562 - val_loss: 0.4218\n",
            "Epoch 15/15\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 2s/step - accuracy: 0.7819 - loss: 0.5783 - val_accuracy: 0.8250 - val_loss: 0.4507\n",
            "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 2s/step - accuracy: 0.8662 - loss: 0.4143\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.8615 - loss: 0.3574\n",
            "\n",
            "Model Performance:\n",
            "Training Accuracy: 85.88%\n",
            "Training Loss: 0.4230\n",
            "Validation Accuracy: 80.31%\n",
            "Validation Loss: 0.4999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from PIL import Image  # Import PIL for image handling\n",
        "\n",
        "# Load the trained model and class names\n",
        "model = load_model('garbage_classifier_final.keras')\n",
        "with open('class_names.txt', 'r') as f:\n",
        "    class_names = [line.strip() for line in f]\n",
        "\n",
        "def preprocess_image(img):\n",
        "    \"\"\"Preprocess the uploaded image for the model\"\"\"\n",
        "    img = img.resize((224, 224))  # Resize to match model input\n",
        "    img_array = image.img_to_array(img) / 255.0  # Normalize\n",
        "    return np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
        "\n",
        "def predict(image):\n",
        "    \"\"\"Make prediction on the input image\"\"\"\n",
        "    processed_img = preprocess_image(image)\n",
        "    predictions = model.predict(processed_img)\n",
        "    predicted_class = class_names[np.argmax(predictions)]\n",
        "    confidence = float(np.max(predictions)) * 100  # Convert to percentage\n",
        "\n",
        "    # Return a dictionary for better display\n",
        "    return {class_names[i]: float(predictions[0][i]) * 100\n",
        "            for i in range(len(class_names))}\n",
        "\n",
        "# Create Gradio interface with updated syntax\n",
        "iface = gr.Interface(\n",
        "    fn=predict,\n",
        "    inputs=gr.Image(type=\"pil\", label=\"Upload Garbage Image\"),\n",
        "    outputs=gr.Label(label=\"Classification Results\"),\n",
        "    title=\"♻️ Garbage Classification AI 🚮\",\n",
        "    description=\"Upload an image of garbage to classify it into one of these categories: \" + \", \".join(class_names),\n",
        "    examples=[\n",
        "        [\"plastic_bottle.jpg\"],\n",
        "        [\"cardboard_box.jpg\"],\n",
        "    ],\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch(share=True)  # share=True generates a public link\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "eBCK6PljJ3cj",
        "outputId": "3177dcf6-6943-4f3d-f583-842be86a2037"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/interface.py:416: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://5ef12303d62de62d3d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5ef12303d62de62d3d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 📝 Summary (optional but useful)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "li_TsUPgSqjI",
        "outputId": "9b579f3b-4163-479d-df6c-c273c1c641dd"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
              "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_2      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m655,872\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │         \u001b[38;5;34m3,591\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d_2      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">655,872</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,591</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,236,375\u001b[0m (16.16 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,236,375</span> (16.16 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m659,463\u001b[0m (2.52 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">659,463</span> (2.52 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m1,318,928\u001b[0m (5.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,318,928</span> (5.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UABe8vtXYBOJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}